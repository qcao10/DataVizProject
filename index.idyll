[meta title:"That's not fair!" description:"An interactive guide to hypothesis testing" /]

[Header
  fullWidth:true
  title:"That's not fair!"
  subtitle:"An interactive guide to hypothesis testing"
  author:"QKE A3"
  authorLink:"https://idyll-lang.org"
  date:`(new Date()).toDateString()`
  background:"#222222"
  color:"#ffffff"
   /]

## Introduction

Few things in life are what we'd call __simple__, but hopefully we can all agree, one of the simplest kind of scenarios we can imagine that isn't just constant, is something with "just two outcomes". When we're only consiering logic (or strong feelings), we can usually simplify most things into combinations of many different factors that are just true and false. If you get a cup of coffee, do you want to add sugar or not? Do you add creamer or skip the dairy? A easy common place example might be, when you flip a coin is it heads or tails? 

[Coin/]

These types of mutually exclusive binary outcomes are sometimes called __Bernoulli random variables__. The simplest binomial distribution, with only one trail, is also called the __Bernoulli distribution__.

While many people don't place important life decisions in the hands of a coin flip, you definitely want to have more confidence than a coin flip when it comes to decisions like if you or someone else is "innocent" or "guilty"!

## The Binomial Count

[var name:"hardcoded_trials" value:4/]

For this distribution, any order of results counts as a combination for that many Heads. e.g. H, H, T, T and H, T, H, T both count as two heads and two tails. So, more common counts of outcomes have higher probabilities. The red bar highlights how common the selected outcome is for [Display value:hardcoded_trials format:"d" /] flips (aka trials). 

[var name:"success" value:0 /]
Number of Flips: [Display value:hardcoded_trials format:"d" /]

Number of Heads: [Display value:success format:"d" /]

Example Flips: [CoinFlips n:4 k:success duration:0.25 /] 

[Binomial n:4 k: success /]

### The Binomial Distribution

As the number of trials increases, so does the probability of ways to get "half and half" outcomes. This makes plenty of sense. If there are 50 flips, we could see 25 heads in many different orders, but getting all heads or all tails only happens 1 way.

[var name:"trials" value:4 /]
[var name:"success2" value:0 /]


Number of Flips:  

[Range min:2 max:14 value:trials interval:1 format:"d" /] [Display value:trials format:"d" /]

Number of Heads:  
[Display value:success2 format:"d" /]

[CoinFlips n:trials size: 50 k:success2 duration:0.25 /]

[Binomial n:trials k:success2 /]

Normally, we expect a coin to be "fair", "equally weighted", or unbaised. In terms of a coin, this just means it really has an equal chance of coming up heads or tails. It can however come up with any combination of heads or tails for a given number of trials.

### Skewed Distributions (That's Unfair!)

However, we know from experience that sometimes things are biased. Sometimes other people want an advantage, they have preconcieved notions, or they've had bad experiences in the past. In these sorts of situations, the probability of some specific outcome may be more likely than equal ("fair").

[var name:"trials2" value:8 /]
Number of Flips: 
[Range min:2 max:40 value:trials2 interval:1 format: "d" /] [Display value:trials2 format:"d" /]


[var name:"binom_mean" value:0.5 /]
Bias (Probability of Success): [Range min:0.1 max: 0.9 value:binom_mean step: 0.1 /] [Display value:binom_mean /]
[Binomial n: trials2 mean: binom_mean/]

### Being confident!

[var name:"trials3" value:8 /]
Number of Flips: [Range min:2 max:40 value: trials3 interval:1 format: "d" /] [Display value: trials3 format:"d" /]

[var name:"binom_mean_h_0" value:0.4 /]
[var name:"binom_mean_h_a" value:0.6 /]

[Equation]H_0[/Equation] Bias: [Range min:0 max: 1 value: binom_mean_h_0 step: 0.1 /] [Display value: binom_mean_h_0 /] 

[Equation]H_a[/Equation] Bias: [Range min:0 max: 1 value: binom_mean_h_a step: 0.1 /] [Display value:binom_mean_h_a /] 

[Binomial n: trials3 mean: binom_mean_h_0 mean_h_a: binom_mean_h_a/]


As you follow along, we'll be exploring a basic, but very important concept: Hypothesis testing. 

First, let us look at how data sample is generated. Below is an empirical histogram of 
a sample, whose data generating process is a normal distribution. 

Please feel free to update the parameters to see how the sample changes. 

Pay careful attention to the next graph. It shows the **sampling distribution** of the sample mean, not the binomial distribution we just looked. The binomial distribution is what the samples are drawn from, and the sampling distribution is more like the values look when we actually encounter them randomly. In both cases, we can see there are peaks, where some things are more common than others, and the extreme cases are rare.

As we randomly sample from any distribution, the resulting distribution approaches the normal distribution the more samples we take of it.

In other words, the **Central Limit Theorem** states that the sample mean will converge to a Normal distribution 
of [Equation]N (\mu = 0, s.e = \frac{\sigma}{\sqrt{n}})[/Equation], where s.e is the standard error or just the standard deviation of the sample mean's distribution. 

As we scale up sample size, we can see that we achieve much tighter range of sample mean. In 
other words, the sample mean estimation is more accurate. 

[var name:"mean" value:0 /]
[Range min:-1 max:1 value:mean step:0.5/]
Mean: [Display value:mean /]

[var name:"sd" value:1 /]
[Range min:0.25 max:2 value:sd step:0.25 /]
Std. Deviation: [Display value:sd /]

[var name:"samplesize" value:1000 /]
[Range min:1000 max:5000 value:samplesize step:100/]
Sample size: [Display value:samplesize /]

[var name:"state1" value:0 /]

[Hist stat:state1 mean:mean sd:sd samplesize:samplesize/]
[button onClick:`state1++`]
  Resample
[/button]

[MeanHist mean:mean sd:sd samplesize:samplesize/]

## Hypothesis testing using quantile versus t-statistics:

So we have learned that the bigger sample size will give a narrower sample mean distribution. How should we proceed next? Let's say below is the sampling distribution of a sample mean centering at 0 and s.e is 1. At a fixed tail percentage, we have an according quantile value. In a 2-tail test, we call them left quantile and right quantile. 


[var name:"alpha" value:0.05/]
[Range min:0.05 max:0.1 value:alpha step:0.025 /]
Tail percentage: [Display value:`alpha*100` /]%
[GaussianCurve alpha:alpha/]

## Hypothesis testing using p-value:

Let's say we have a t-statistics that yields [Equation]p-value = 10\%[/Equation]. This means, under the Null hypothesis, we only have 10% to achieve such a statistics or more extreme than that. What does it even mean? Let's generate more samples (and more statistics) to verify that. 
[var name:"state3" value:0 /]
[var name:"reject" value:0 /]
[var name:"playG2" value:0/]

[GaussianCurve2 state:state3 reject:reject play:playG2/]
[button onClick:`state3++`]
  Generate a statistic
[/button]
[button onClick:`playG2 = 1-playG2`]
[Display var:`playG2 === 0 ? '▶' : '❙❙'` /]
[/button]
Number of statistics drawn: [Display value:state3 /]


## Type I and Type II error

Below we are showing the distribution of the data under the Null and
Alternative Hypotheses.

[var name:"state2" value:0 /]
[CustomD3Component className:"d3-component" state:state2 mean:mean sd:1 samplesize:samplesize/]
[button onClick:`state2++`]
  Click Me.
[/button]


## Acknowledgement

This page is powered by Idyll at https://idyll-lang.org/docs/,
join the [chatroom](https://gitter.im/idyll-lang/Lobby), or see the project on [GitHub](https://github.com/idyll-lang/idyll).
